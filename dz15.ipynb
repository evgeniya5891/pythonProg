{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Домашнее задание к лекции \"Основы веб-скрапинга и работы с API\"\n",
    "Задание 1.\n",
    "Обязательная часть\n",
    "Будем парсить страницу со свежеми новостям на habr.com/ru/all/.\n",
    "\n",
    "Вам необходимо собирать только те статьи, в которых встречается хотя бы одно требуемое ключевое слово. Эти слова определяем в начале кода в переменной, например:\n",
    "\n",
    "KEYWORDS = ['python', 'парсинг']\n",
    "\n",
    "Поиск вести по всей доступной preview-информации (это информация, доступная непосредственно с текущей страницы).\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: - - ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>hub_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>сегодня в 12:48</td>\n",
       "      <td>DigitalEcosystems</td>\n",
       "      <td>Дайджест интересных материалов для мобильного ...</td>\n",
       "      <td>В этом дайджесте — обновления Android, собстве...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>вчера в 22:41</td>\n",
       "      <td>bymytry</td>\n",
       "      <td>Токсичность Белорусского IT аутсорса</td>\n",
       "      <td>ПредисловиеЯ прекрасно понимаю что описываемые...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date             author  \\\n",
       "0  сегодня в 12:48  DigitalEcosystems   \n",
       "0    вчера в 22:41            bymytry   \n",
       "\n",
       "                                               title  \\\n",
       "0  Дайджест интересных материалов для мобильного ...   \n",
       "0               Токсичность Белорусского IT аутсорса   \n",
       "\n",
       "                                            hub_text  \n",
       "0  В этом дайджесте — обновления Android, собстве...  \n",
       "0  ПредисловиеЯ прекрасно понимаю что описываемые...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KEYWORDS = ['python', 'парсинг']\n",
    "req = requests.get('https://habr.com/ru/all/')\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "news_blocks = soup.find_all('article', class_='post')\n",
    "articles_intro = list(map(lambda x: x.find('h2', class_='post__title'), news_blocks))\n",
    "a_list = list(map(lambda x: x.find('a').get('href'), articles_intro))\n",
    "kom_news = pd.DataFrame()\n",
    "for link in a_list:\n",
    "    soup_1 = BeautifulSoup(requests.get(link).text, 'html.parser')\n",
    "    time.sleep(0.3)\n",
    "    #hub  = requests.get(link).text.lower()\n",
    "    #print(title_element.text, title_element.attrs.get('href'))\n",
    "    hub_text = soup_1.find(\"div\", class_='post__text').text\n",
    "    if any([desired in hub_text for desired in KEYWORDS]):\n",
    "        #hub_text = soup_1.find(\"div\", class_='post__text').text\n",
    "        title = soup_1.find('span', class_='post__title-text').text\n",
    "        date = soup_1.find('span', class_='post__time').text\n",
    "        author = soup_1.find('span', class_='user-info__nickname').text\n",
    "        row = {'date': date, 'author': author, 'title': title, 'hub_text': hub_text}\n",
    "        kom_news = pd.concat([kom_news, pd.DataFrame([row])])\n",
    "kom_news       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 2.\n",
    "Обязательная часть¶\n",
    "Написать скрипт, который будет проверять список e-mail адресов на утечку при помощи сервиса Avast Hack Ckeck. Список email-ов задаем переменной в начале кода:\n",
    "EMAIL = [xxx@x.ru, yyy@y.com]\n",
    "\n",
    "В итоге должен формироваться датафрейм со столбцами: - <дата утечки> - <источник утечки> - <описание утечки>.\n",
    "\n",
    "Подсказка: сервис работает при помощи \"скрытого\" API. Внимательно изучите post-запросы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publishDate</th>\n",
       "      <th>site</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-28T00:00:00Z</td>\n",
       "      <td>verifications.io</td>\n",
       "      <td>Big data e-mail verification platform verifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-21T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>At some time in 2020, the Russian social netwo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-21T00:00:00Z</td>\n",
       "      <td>www.dangdang.com</td>\n",
       "      <td>This is a list of email addresses only, and as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-03T00:00:00Z</td>\n",
       "      <td>azcentral.com</td>\n",
       "      <td>At an unconfirmed date, online Arizona newspap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-28T00:00:00Z</td>\n",
       "      <td>wishbone.io</td>\n",
       "      <td>In January 2020, the online poll website Wishb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-04T00:00:00Z</td>\n",
       "      <td>myheritage.com</td>\n",
       "      <td>In October 2017, a customer database belonging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-01T00:00:00Z</td>\n",
       "      <td>creocommunity.com</td>\n",
       "      <td>At an unconfirmed date, Creo Community's user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-06-13T00:00:00Z</td>\n",
       "      <td>canva.com</td>\n",
       "      <td>In May 2019, graphic-design site Canva's datab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>parapa.mail.ru</td>\n",
       "      <td>In July and August 2016, two criminals execute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-24T00:00:00Z</td>\n",
       "      <td>dropbox.com</td>\n",
       "      <td>Cloud storage company Dropbox suffered a major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>linkedin.com</td>\n",
       "      <td>In 2012, online professional networking platfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-01T00:00:00Z</td>\n",
       "      <td>rayli.com.cn</td>\n",
       "      <td>On an unconfirmed date, Chinese gossip site Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-17T00:00:00Z</td>\n",
       "      <td>zynga.com</td>\n",
       "      <td>In September 2019, the game developer Zynga wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-29T00:00:00Z</td>\n",
       "      <td>vk.com</td>\n",
       "      <td>Popular Russian social networking platform VKo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-11T00:00:00Z</td>\n",
       "      <td>medicaresupplement.com</td>\n",
       "      <td>In May 2019, a security researcher discovered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-21T00:00:00Z</td>\n",
       "      <td>adobe.com</td>\n",
       "      <td>In October of 2013, criminals penetrated Adobe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-18T00:00:00Z</td>\n",
       "      <td>netlog.com</td>\n",
       "      <td>Netlog (formerly known as Facebox and Bingbox)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-10T00:00:00Z</td>\n",
       "      <td>na</td>\n",
       "      <td>This is a compilation of files including breac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-15T00:00:00Z</td>\n",
       "      <td>globalreach.eu</td>\n",
       "      <td>In 2016, Global Reach Technology's database wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-02-14T00:00:00Z</td>\n",
       "      <td>cfire.mail.ru</td>\n",
       "      <td>In July and August of 2016, two criminals carr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-31T00:00:00Z</td>\n",
       "      <td>cdprojektred.com</td>\n",
       "      <td>In March 2016, CDProjektRed.com.com's forum da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-10-23T00:00:00Z</td>\n",
       "      <td>imesh.com</td>\n",
       "      <td>In June 2016, a cache of over 51 million user ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-03-24T00:00:00Z</td>\n",
       "      <td>youku.com</td>\n",
       "      <td>Youku is a large Chinese video content company...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            publishDate                    site  \\\n",
       "0  2019-03-28T00:00:00Z        verifications.io   \n",
       "0  2020-05-21T00:00:00Z                  vk.com   \n",
       "0  2019-02-21T00:00:00Z        www.dangdang.com   \n",
       "0  2020-01-03T00:00:00Z           azcentral.com   \n",
       "0  2020-05-28T00:00:00Z             wishbone.io   \n",
       "0  2017-11-04T00:00:00Z          myheritage.com   \n",
       "0  2017-12-01T00:00:00Z       creocommunity.com   \n",
       "0  2019-06-13T00:00:00Z               canva.com   \n",
       "0  2017-02-14T00:00:00Z          parapa.mail.ru   \n",
       "0  2016-10-24T00:00:00Z             dropbox.com   \n",
       "0  2016-10-21T00:00:00Z            linkedin.com   \n",
       "0  2017-03-01T00:00:00Z            rayli.com.cn   \n",
       "0  2019-10-17T00:00:00Z               zynga.com   \n",
       "0  2016-10-29T00:00:00Z                  vk.com   \n",
       "0  2019-07-11T00:00:00Z  medicaresupplement.com   \n",
       "0  2016-10-21T00:00:00Z               adobe.com   \n",
       "0  2018-02-18T00:00:00Z              netlog.com   \n",
       "0  2020-09-10T00:00:00Z                      na   \n",
       "0  2017-03-15T00:00:00Z          globalreach.eu   \n",
       "0  2017-02-14T00:00:00Z           cfire.mail.ru   \n",
       "0  2017-01-31T00:00:00Z        cdprojektred.com   \n",
       "0  2016-10-23T00:00:00Z               imesh.com   \n",
       "0  2017-03-24T00:00:00Z               youku.com   \n",
       "\n",
       "                                         description  \n",
       "0  Big data e-mail verification platform verifica...  \n",
       "0  At some time in 2020, the Russian social netwo...  \n",
       "0  This is a list of email addresses only, and as...  \n",
       "0  At an unconfirmed date, online Arizona newspap...  \n",
       "0  In January 2020, the online poll website Wishb...  \n",
       "0  In October 2017, a customer database belonging...  \n",
       "0  At an unconfirmed date, Creo Community's user ...  \n",
       "0  In May 2019, graphic-design site Canva's datab...  \n",
       "0  In July and August 2016, two criminals execute...  \n",
       "0  Cloud storage company Dropbox suffered a major...  \n",
       "0  In 2012, online professional networking platfo...  \n",
       "0  On an unconfirmed date, Chinese gossip site Ra...  \n",
       "0  In September 2019, the game developer Zynga wa...  \n",
       "0  Popular Russian social networking platform VKo...  \n",
       "0  In May 2019, a security researcher discovered ...  \n",
       "0  In October of 2013, criminals penetrated Adobe...  \n",
       "0  Netlog (formerly known as Facebox and Bingbox)...  \n",
       "0  This is a compilation of files including breac...  \n",
       "0  In 2016, Global Reach Technology's database wa...  \n",
       "0  In July and August of 2016, two criminals carr...  \n",
       "0  In March 2016, CDProjektRed.com.com's forum da...  \n",
       "0  In June 2016, a cache of over 51 million user ...  \n",
       "0  Youku is a large Chinese video content company...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMAIL = ['xxx@x.ru', 'yyy@y.com']\n",
    "URL = 'https://identityprotection.avast.com/v1/web/query/site-breaches/unauthorized-data'\n",
    "headers = {'Vaar-Version':'0', 'Vaar-Header-App-Product':'hackcheck-web-avast'}\n",
    "params = {'emailAddresses': EMAIL }\n",
    "leak_info = pd.DataFrame()\n",
    "req = requests.post(url = URL, data = json.dumps(params), headers=headers)\n",
    "req.text\n",
    "list_ = json.loads(req.text)\n",
    "list_\n",
    "for info in list_['breaches']:\n",
    "    site = list_['breaches'][info]['site']\n",
    "    publishDate = list_['breaches'][info]['publishDate']\n",
    "    description = list_['breaches'][info]['description']\n",
    "    row = {'publishDate': publishDate, 'site': site, 'description': description}\n",
    "    leak_info = pd.concat([leak_info, pd.DataFrame([row])])\n",
    "leak_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
